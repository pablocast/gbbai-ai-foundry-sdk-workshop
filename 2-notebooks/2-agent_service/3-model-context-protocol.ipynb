{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ù§Ô∏è Azure AI Agents\n",
    "\n",
    "## Model Context Protocol (MCP) lab\n",
    "![flow](../../utils/model-context-protocol.gif)\n",
    "\n",
    "Playground to experiment the [Model Context Protocol](https://modelcontextprotocol.io/).\n",
    "\n",
    "This lab includes the following MCP servers:\n",
    "- Basic weather service: provide tools to get cities for a given country and retrieve random weather information for a specified city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='testconnection'></a>\n",
    "### üß™ Test the connection to the MCP servers and List Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to server https://aca-weather-jrpn.yellowmoss-9594982e.eastus.azurecontainerapps.io/weather/sse\n",
      "‚öôÔ∏è Tools:\n",
      "  - get_cities\n",
      "     Input Schema: {'properties': {'country': {'title': 'Country', 'type': 'string'}}, 'required': ['country'], 'title': 'get_citiesArguments', 'type': 'object'}\n",
      "  - get_weather\n",
      "     Input Schema: {'properties': {'city': {'title': 'City', 'type': 'string'}}, 'required': ['city'], 'title': 'get_weatherArguments', 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "import os, json, asyncio, sys\n",
    "\n",
    "sys.path.insert(1, \"../../shared\")  # add the shared directory to the Python path\n",
    "import utils\n",
    "from mcp import ClientSession\n",
    "from mcp.client.sse import sse_client\n",
    "import nest_asyncio\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime as pydatetime\n",
    "from azure.ai.inference.models import UserMessage, SystemMessage\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "server_url = \"https://\" + f\"{os.getenv('WEATHER_CONTAINERAPP_URL')}/weather/sse\"\n",
    "\n",
    "async def list_tools(server_url, authorization_header=None):\n",
    "    headers = {\"Authorization\": authorization_header} if authorization_header else None\n",
    "    async with sse_client(server_url, headers) as streams:\n",
    "        async with ClientSession(streams[0], streams[1]) as session:\n",
    "            await session.initialize()\n",
    "\n",
    "            response = await session.list_tools()\n",
    "            tools = response.tools\n",
    "    return tools\n",
    "\n",
    "tools = await list_tools(f\"{server_url}\")\n",
    "\n",
    "print(f\"‚úÖ Connected to server {server_url}\")\n",
    "print(\"‚öôÔ∏è Tools:\")\n",
    "for tool in tools:\n",
    "    print(f\"  - {tool.name}\")\n",
    "    print(f\"     Input Schema: {tool.inputSchema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Run an OpenAI completion with MCP tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Projects\n",
    "credential = DefaultAzureCredential()\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=credential, conn_str=os.environ[\"PROJECT_CONNECTION_STRING\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to server https://aca-weather-jrpn.yellowmoss-9594982e.eastus.azurecontainerapps.io/weather/sse\n",
      "‚ñ∂Ô∏è Step 1: start a completion to identify the appropriate functions to invoke based on the prompt\n",
      "‚ñ∂Ô∏è Step 2: call the functions\n",
      "   Function Name: get_weather Function Args: {'city': 'Medellin'}\n",
      "   Function response: [TextContent(type='text', text=\"{'city': 'Medellin', 'condition': 'Windy', 'temperature': 28.23, 'humidity': 66.06}\", annotations=None)]\n",
      "‚ñ∂Ô∏è Step 3: finish with a completion to anwser the user prompt using the function response\n",
      "üí¨ The current weather in Medellin is windy with a temperature of approximately 28.23¬∞C and a humidity level of 66.06%.\n"
     ]
    }
   ],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "async def call_tool(mcp_session, function_name, function_args):\n",
    "    try:\n",
    "        func_response = await mcp_session.call_tool(function_name, function_args)\n",
    "        func_response_content = func_response.content\n",
    "    except Exception as e:\n",
    "        func_response_content = json.dumps({\"error\": str(e)})\n",
    "    return str(func_response_content)\n",
    "\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a helpful enterprise assistant at Contoso. \"\n",
    "    f\"Today's date is {pydatetime.now().strftime('%A, %b %d, %Y, %I:%M %p')}. \"\n",
    "    \"You have access to tools to get information about the weather, \"\n",
    ")\n",
    "\n",
    "\n",
    "async def run_completion_with_tools(server_url, user_input):\n",
    "    async with sse_client(server_url) as streams:\n",
    "        async with ClientSession(streams[0], streams[1]) as session:\n",
    "            await session.initialize()\n",
    "            response = await session.list_tools()\n",
    "            tools = response.tools\n",
    "            print(f\"‚úÖ Connected to server {server_url}\")\n",
    "            openai_tools = [\n",
    "                {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\"name\": tool.name, \"parameters\": tool.inputSchema},\n",
    "                }\n",
    "                for tool in tools\n",
    "            ]\n",
    "\n",
    "            # Step 1: send the conversation and available functions to the model\n",
    "            print(\n",
    "                \"‚ñ∂Ô∏è Step 1: start a completion to identify the appropriate functions to invoke based on the prompt\"\n",
    "            )\n",
    "            messages = [{\"role\": \"user\", \"content\": user_input}]\n",
    "            with project_client.inference.get_chat_completions_client() as chat_client:\n",
    "                system_msg = SystemMessage(content=system_prompt)\n",
    "                user_msg = UserMessage(content=user_input)\n",
    "\n",
    "                response = chat_client.complete(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=[system_msg, user_msg],\n",
    "                    tools=openai_tools,\n",
    "                )\n",
    "\n",
    "                response_message = response.choices[0].message\n",
    "                tool_calls = response_message.tool_calls\n",
    "                if tool_calls:\n",
    "                    # Step 2: call the function\n",
    "                    messages.append(\n",
    "                        response_message\n",
    "                    )  # extend conversation with assistant's reply\n",
    "                    # send the info for each function call and function response to the model\n",
    "                    print(\"‚ñ∂Ô∏è Step 2: call the functions\")\n",
    "                    for tool_call in tool_calls:\n",
    "                        function_name = tool_call.function.name\n",
    "                        function_args = json.loads(tool_call.function.arguments)\n",
    "                        print(\n",
    "                            f\"   Function Name: {function_name} Function Args: {function_args}\"\n",
    "                        )\n",
    "\n",
    "                        function_response = await call_tool(\n",
    "                            session, function_name, function_args\n",
    "                        )\n",
    "                        # Add the tool response\n",
    "                        print(f\"   Function response: {function_response}\")\n",
    "                        messages.append(\n",
    "                            {\n",
    "                                \"tool_call_id\": tool_call.id,\n",
    "                                \"role\": \"tool\",\n",
    "                                \"name\": function_name,\n",
    "                                \"content\": function_response,\n",
    "                            }\n",
    "                        )  # extend conversation with function response\n",
    "                    print(\n",
    "                        \"‚ñ∂Ô∏è Step 3: finish with a completion to anwser the user prompt using the function response\"\n",
    "                    )\n",
    "                    # Step 3: finish with a completion to answer the user prompt using the function response\n",
    "                    second_response = chat_client.complete(\n",
    "                        model=\"gpt-4o\",\n",
    "                        messages=messages,\n",
    "                    )  # get a new response from the model where it can see the function response\n",
    "                    print(\"üí¨\", second_response.choices[0].message.content)\n",
    "\n",
    "\n",
    "asyncio.run(\n",
    "    run_completion_with_tools(f\"{server_url}\", \"What's the current weather in Medellin?\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
