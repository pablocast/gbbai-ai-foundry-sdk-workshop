{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee0c0ebe",
   "metadata": {},
   "source": [
    "# üèãÔ∏è‚Äç‚ôÄÔ∏è Health & Fitness Evaluations with Azure AI Foundry üèãÔ∏è‚Äç‚ôÇÔ∏è\n",
    "\n",
    "This notebook demuestra c√≥mo **evaluar** un modelo de IA generativa (o aplicaci√≥n) utilizando el ecosistema de **Azure AI Foundry**. Destacaremos tres SDK de Python clave:\n",
    "1. **azure-ai-projects** (AIProjectClient): gestionar y orquestar evaluaciones en la nube.\n",
    "2. **azure-ai-inference**: realizar inferencia de modelos (opcional pero √∫til para generar datos para evaluaci√≥n).\n",
    "3. **azure-ai-evaluation**: ejecutar m√©tricas automatizadas para la calidad y seguridad de la salida de modelos de lenguaje.\n",
    "\n",
    "Crearemos o utilizaremos algunos datos sint√©ticos de preguntas y respuestas en salud y fitness, y luego mediremos qu√© tan bien responde tu modelo. Realizaremos evaluaciones tanto **locales** como **en la nube** (en un proyecto de Azure AI Foundry).\n",
    "\n",
    "> **Descargo de responsabilidad**: Esto aborda un escenario hipot√©tico de salud y fitness. **No se proporciona consejo m√©dico real**. Consulta siempre a profesionales.\n",
    "\n",
    "## Contenidos del cuaderno\n",
    "1. [Configuraci√≥n e Importaciones](#1-Setup-and-Imports)\n",
    "2. [Ejemplos de Evaluaci√≥n Local](#3-Local-Evaluation)\n",
    "3. [Evaluaci√≥n en la Nube con AIProjectClient](#4-Cloud-Evaluation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfadf84",
   "metadata": {
    "id": "1-Setup-and-Imports"
   },
   "source": [
    "Instalaremos las bibliotecas necesarias, las importaremos y definiremos algunos datos sint√©ticos. \n",
    "\n",
    "### Dependencias\n",
    "- `azure-ai-projects` para orquestar evaluaciones en tu Proyecto Azure AI Foundry.\n",
    "- `azure-ai-evaluation` para m√©tricas integradas o personalizadas (como Relevancia, Fundamentaci√≥n, F1Score, etc.).\n",
    "- `azure-ai-inference`(opcional) si deseas generar completions para producir datos a evaluar.\n",
    "- `azure-identity` (para la autenticaci√≥n con Azure mediante DefaultAzureCredential).\n",
    "\n",
    "### Datos Sint√©ticos\n",
    "Crearemos un peque√±o archivo JSONL con pares de Preguntas y Respuestas de salud y fitness, que incluyen query, response, context y ground_truth. Esto simula un escenario en el que tenemos preguntas de usuarios, las respuestas del modelo y adem√°s una referencia de la verdad.\n",
    "\n",
    "Puedes adaptar este enfoque a cualquier dominio: por ejemplo, finanzas, comercio electr√≥nico, etc.\n",
    "\n",
    "<img src=\"./seq-diagrams/2-evals.png\" alt=\"Flujo de Evaluaci√≥n\" width=\"30%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b889daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# If you need to install these, uncomment:\n",
    "# !pip install azure-ai-projects azure-ai-evaluation azure-ai-inference azure-identity\n",
    "# !pip install opentelemetry-sdk azure-core-tracing-opentelemetry  # optional for advanced tracing\n",
    "\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# We'll create a synthetic dataset in JSON Lines format\n",
    "synthetic_eval_data = [\n",
    "    {\n",
    "        \"query\": \"How can I start a beginner workout routine at home?\",\n",
    "        \"context\": \"Workout routines can include push-ups, bodyweight squats, lunges, and planks.\",\n",
    "        \"response\": \"You can just go for 10 push-ups total.\",\n",
    "        \"ground_truth\": \"At home, you can start with short, low-intensity workouts: push-ups, lunges, planks.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Are diet sodas healthy for daily consumption?\",\n",
    "        \"context\": \"Sugar-free or diet drinks may reduce sugar intake, but they still contain artificial sweeteners.\",\n",
    "        \"response\": \"Yes, diet sodas are 100% healthy.\",\n",
    "        \"ground_truth\": \"Diet sodas have fewer sugars than regular soda, but 'healthy' is not guaranteed due to artificial additives.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the capital of France?\",\n",
    "        \"context\": \"France is in Europe. Paris is the capital.\",\n",
    "        \"response\": \"London.\",\n",
    "        \"ground_truth\": \"Paris.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Write them to a local JSONL file\n",
    "eval_data_path = Path(\"./health_fitness_eval_data.jsonl\")\n",
    "with eval_data_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for row in synthetic_eval_data:\n",
    "        f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "print(f\"Sample evaluation data written to {eval_data_path.resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d5598",
   "metadata": {
    "id": "3-Local-Evaluation"
   },
   "source": [
    "# 3. Local Evaluation Examples\n",
    "\n",
    "Mostraremos c√≥mo ejecutar una evaluaci√≥n local basada en c√≥digo en un conjunto de datos JSONL. Vamos a:\n",
    "1. **Cargar** los datos.\n",
    "2. **Definir** uno o m√°s evaluadores. (por ejemplo, F1ScoreEvaluator, RelevanceEvaluator, GroundednessEvaluator o personalizados).\n",
    "3. **Ejecutar** evaluate(...) para producir un diccionario de m√©tricas.\n",
    "\n",
    "> Tambi√©n podemos trabajar con datos de conversaciones en m√∫ltiples turnos o agregar columnas adicionales, como ground_truth, para m√©tricas avanzadas.\n",
    "\n",
    "## Ejemplo 1: Combinando F1Score, Relevance y Groundedness\n",
    "Combinaremos:\n",
    "- F1ScoreEvaluator (basado en NLP, que compara response con ground_truth)\n",
    "- RelevanceEvaluator (asistido por IA, que utiliza GPT para juzgar qu√© tan bien response responde a query)\n",
    "- GroundednessEvaluator (que verifica qu√© tan bien la respuesta est√° fundamentada en el context proporcionado)\n",
    "- Un evaluador personalizado basado en c√≥digo que registra la longitud de la respuesta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9f04f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Could not import RedTeam. Please install the dependency with `pip install azure-ai-evaluation[redteam]`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-04-07 14:08:45 -0300][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-04-07 14:08:45 -0300][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-04-07 14:08:45 -0300][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-04-07 14:08:46 -0300][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kxxkprsf_20250407_140845_430440, log path: C:\\Users\\pablocastao\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kxxkprsf_20250407_140845_430440\\logs.txt\n",
      "[2025-04-07 14:08:46 -0300][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_fydn8m18_20250407_140845_431955, log path: C:\\Users\\pablocastao\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_fydn8m18_20250407_140845_431955\\logs.txt\n",
      "[2025-04-07 14:08:46 -0300][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_vx4ex3q4_20250407_140845_431955, log path: C:\\Users\\pablocastao\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_vx4ex3q4_20250407_140845_431955\\logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-07 14:08:46 -0300    3748 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-04-07 14:08:46 -0300    3748 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-04-07 14:08:46 -0300    3748 execution.bulk     INFO     Average execution time for completed lines: 0.02 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_fydn8m18_20250407_140845_431955\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-04-07 14:08:45.426850-03:00\"\n",
      "Duration: \"0:00:02.130886\"\n",
      "Output path: \"C:\\Users\\pablocastao\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_fydn8m18_20250407_140845_431955\"\n",
      "\n",
      "2025-04-07 14:09:08 -0300    3748 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-04-07 14:09:08 -0300    3748 execution.bulk     INFO     Average execution time for completed lines: 21.94 seconds. Estimated time for incomplete lines: 43.88 seconds.\n",
      "2025-04-07 14:09:08 -0300    3748 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-04-07 14:09:08 -0300    3748 execution.bulk     INFO     Average execution time for completed lines: 22.12 seconds. Estimated time for incomplete lines: 44.24 seconds.\n",
      "2025-04-07 14:09:08 -0300    3748 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-04-07 14:09:08 -0300    3748 execution.bulk     INFO     Average execution time for completed lines: 11.35 seconds. Estimated time for incomplete lines: 11.35 seconds.\n",
      "2025-04-07 14:09:09 -0300    3748 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-04-07 14:09:09 -0300    3748 execution.bulk     INFO     Average execution time for completed lines: 11.47 seconds. Estimated time for incomplete lines: 11.47 seconds.\n",
      "2025-04-07 14:09:09 -0300    3748 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-04-07 14:09:09 -0300    3748 execution.bulk     INFO     Average execution time for completed lines: 7.75 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-04-07 14:09:09 -0300    3748 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-04-07 14:09:09 -0300    3748 execution.bulk     INFO     Average execution time for completed lines: 7.77 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-04-07 14:08:46 -0300    3748 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-04-07 14:09:08 -0300    3748 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-04-07 14:09:08 -0300    3748 execution.bulk     INFO     Average execution time for completed lines: 22.12 seconds. Estimated time for incomplete lines: 44.24 seconds.\n",
      "2025-04-07 14:09:08 -0300    3748 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-04-07 14:09:08 -0300    3748 execution.bulk     INFO     Average execution time for completed lines: 11.35 seconds. Estimated time for incomplete lines: 11.35 seconds.\n",
      "2025-04-07 14:09:09 -0300    3748 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-04-07 14:09:09 -0300    3748 execution.bulk     INFO     Average execution time for completed lines: 7.75 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kxxkprsf_20250407_140845_430440\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-04-07 14:08:45.427835-03:00\"\n",
      "Duration: \"0:00:24.486146\"\n",
      "Output path: \"C:\\Users\\pablocastao\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kxxkprsf_20250407_140845_430440\"\n",
      "\n",
      "2025-04-07 14:08:46 -0300    3748 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-04-07 14:09:08 -0300    3748 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-04-07 14:09:08 -0300    3748 execution.bulk     INFO     Average execution time for completed lines: 21.94 seconds. Estimated time for incomplete lines: 43.88 seconds.\n",
      "2025-04-07 14:09:09 -0300    3748 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-04-07 14:09:09 -0300    3748 execution.bulk     INFO     Average execution time for completed lines: 11.47 seconds. Estimated time for incomplete lines: 11.47 seconds.\n",
      "2025-04-07 14:09:09 -0300    3748 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-04-07 14:09:09 -0300    3748 execution.bulk     INFO     Average execution time for completed lines: 7.77 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_vx4ex3q4_20250407_140845_431955\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-04-07 14:08:45.426850-03:00\"\n",
      "Duration: \"0:00:24.542062\"\n",
      "Output path: \"C:\\Users\\pablocastao\\.promptflow\\.runs\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_vx4ex3q4_20250407_140845_431955\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"f1_score\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:02.130886\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"C:\\\\Users\\\\pablocastao\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_fydn8m18_20250407_140845_431955\"\n",
      "    },\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:24.486146\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"C:\\\\Users\\\\pablocastao\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_kxxkprsf_20250407_140845_430440\"\n",
      "    },\n",
      "    \"groundedness\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:24.542062\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"C:\\\\Users\\\\pablocastao\\\\.promptflow\\\\.runs\\\\azure_ai_evaluation_evaluators_common_base_eval_asyncevaluatorbase_vx4ex3q4_20250407_140845_431955\"\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "Local evaluation result =>\n",
      "{'rows': [{'inputs.query': 'How can I start a beginner workout routine at home?', 'inputs.context': 'Workout routines can include push-ups, bodyweight squats, lunges, and planks.', 'inputs.response': 'You can just go for 10 push-ups total.', 'inputs.ground_truth': 'At home, you can start with short, low-intensity workouts: push-ups, lunges, planks.', 'outputs.f1_score.f1_score': 0.30000000000000004, 'outputs.f1_score.f1_result': 'fail', 'outputs.f1_score.f1_threshold': 0.5, 'outputs.relevance.relevance': 3, 'outputs.relevance.gpt_relevance': 3, 'outputs.relevance.relevance_reason': 'The response is related but incomplete, as it only suggests one exercise without providing a full routine or additional guidance.', 'outputs.relevance.relevance_result': 'pass', 'outputs.relevance.relevance_threshold': 3, 'outputs.groundedness.groundedness': 3, 'outputs.groundedness.gpt_groundedness': 3, 'outputs.groundedness.groundedness_reason': 'The RESPONSE is related to the CONTEXT but adds unsupported details (the number of push-ups) and omits other exercises mentioned in the CONTEXT.', 'outputs.groundedness.groundedness_result': 'pass', 'outputs.groundedness.groundedness_threshold': 3}, {'inputs.query': 'Are diet sodas healthy for daily consumption?', 'inputs.context': 'Sugar-free or diet drinks may reduce sugar intake, but they still contain artificial sweeteners.', 'inputs.response': 'Yes, diet sodas are 100% healthy.', 'inputs.ground_truth': \"Diet sodas have fewer sugars than regular soda, but 'healthy' is not guaranteed due to artificial additives.\", 'outputs.f1_score.f1_score': 0.2608695652, 'outputs.f1_score.f1_result': 'fail', 'outputs.f1_score.f1_threshold': 0.5, 'outputs.relevance.relevance': 2, 'outputs.relevance.gpt_relevance': 2, 'outputs.relevance.relevance_reason': 'The response attempts to address the question but includes incorrect information.', 'outputs.relevance.relevance_result': 'fail', 'outputs.relevance.relevance_threshold': 3, 'outputs.groundedness.groundedness': 2, 'outputs.groundedness.gpt_groundedness': 2, 'outputs.groundedness.groundedness_reason': 'The RESPONSE directly contradicts the CONTEXT by claiming diet sodas are 100% healthy, which is not supported by the information given.', 'outputs.groundedness.groundedness_result': 'fail', 'outputs.groundedness.groundedness_threshold': 3}, {'inputs.query': \"What's the capital of France?\", 'inputs.context': 'France is in Europe. Paris is the capital.', 'inputs.response': 'London.', 'inputs.ground_truth': 'Paris.', 'outputs.f1_score.f1_score': 0.0, 'outputs.f1_score.f1_result': 'fail', 'outputs.f1_score.f1_threshold': 0.5, 'outputs.relevance.relevance': 2, 'outputs.relevance.gpt_relevance': 2, 'outputs.relevance.relevance_reason': 'The response is factually incorrect as it states that London is the capital of France, which is not true.', 'outputs.relevance.relevance_result': 'fail', 'outputs.relevance.relevance_threshold': 3, 'outputs.groundedness.groundedness': 1, 'outputs.groundedness.gpt_groundedness': 1, 'outputs.groundedness.groundedness_reason': 'The response is completely ungrounded as it introduces a topic (London) that has no connection to the provided CONTEXT.', 'outputs.groundedness.groundedness_result': 'fail', 'outputs.groundedness.groundedness_threshold': 3}], 'metrics': {'f1_score.f1_score': 0.18695652173333333, 'f1_score.f1_threshold': 0.5, 'relevance.relevance': 2.3333333333333335, 'relevance.gpt_relevance': 2.3333333333333335, 'relevance.relevance_threshold': 3.0, 'groundedness.groundedness': 2.0, 'groundedness.gpt_groundedness': 2.0, 'groundedness.groundedness_threshold': 3.0}, 'studio_url': None}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created AIProjectClient.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.ai.evaluation import (\n",
    "    evaluate,\n",
    "    F1ScoreEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    GroundednessEvaluator\n",
    ")\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects.models import ConnectionType\n",
    "\n",
    "# We'll define an example GPT-based config (if we want AI-assisted evaluators). \n",
    "# This is needed for AI-assisted evaluators. Fill with your Azure OpenAI config.\n",
    "# If you skip some evaluators, you can omit.\n",
    "connection_string = os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "        credential=DefaultAzureCredential(),\n",
    "        conn_str=connection_string\n",
    "    )\n",
    "\n",
    "default_connection = project_client.connections.get_default(\n",
    "    connection_type=ConnectionType.AZURE_OPEN_AI\n",
    ")\n",
    "\n",
    "model_config = default_connection.to_evaluator_model_config(\n",
    "    deployment_name='gpt-4o', \n",
    "    api_version='2024-12-01-preview'\n",
    ")\n",
    "model_config['type'] = 'azure_openai'\n",
    "\n",
    "f1_eval = F1ScoreEvaluator()\n",
    "rel_eval = RelevanceEvaluator(model_config=model_config)\n",
    "ground_eval = GroundednessEvaluator(model_config=model_config)\n",
    "\n",
    "# We'll run evaluate(...) with these evaluators.\n",
    "results = evaluate(\n",
    "    data=str(eval_data_path),\n",
    "    evaluators={\n",
    "        \"f1_score\": f1_eval,\n",
    "        \"relevance\": rel_eval,\n",
    "        \"groundedness\": ground_eval,\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"f1_score\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\"\n",
    "            }\n",
    "        },\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        },\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Local evaluation result =>\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d525400",
   "metadata": {},
   "source": [
    "**Inspecting Local Results**\n",
    "\n",
    "The `evaluate(...)` call returns a dictionary with:\n",
    "- **`metrics`**: aggregated metrics across rows (like average F1, Relevance, or Groundedness)\n",
    "- **`rows`**: row-by-row results with inputs and evaluator outputs\n",
    "- **`traces`**: debugging info (if any)\n",
    "\n",
    "You can further analyze these results, store them in a database, or integrate them into your CI/CD pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b903ea",
   "metadata": {
    "id": "4-Cloud-Evaluation"
   },
   "source": [
    "# 4. Cloud Evaluation with `AIProjectClient`\n",
    "\n",
    "Sometimes, we want to:\n",
    "- Evaluate large or sensitive datasets in the cloud (scalability, governed access).\n",
    "- Keep track of evaluation results in an Azure AI Foundry project.\n",
    "- Optionally schedule recurring evaluations.\n",
    "\n",
    "We'll do that by:\n",
    "1. **Upload** the local JSONL to your Azure AI Foundry project.\n",
    "2. **Create** an `Evaluation` referencing built-in or custom evaluator definitions.\n",
    "3. **Poll** until the job is done (with retry logic for resilience).\n",
    "4. **Review** the results in the portal or via `project_client.evaluations.get(...)`.\n",
    "\n",
    "### Prerequisites\n",
    "- An Azure AI Foundry project with a valid **Connection String** (from your project‚Äôs Overview page).\n",
    "- An Azure OpenAI deployment (if using AI-assisted evaluators).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d936ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.ai.projects.models import (\n",
    "    Evaluation, Dataset, EvaluatorConfiguration, ConnectionType\n",
    ")\n",
    "from azure.ai.evaluation import F1ScoreEvaluator, RelevanceEvaluator, ViolenceEvaluator\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.core.exceptions import ServiceResponseError\n",
    "import time\n",
    "\n",
    "# 1) Connect to Azure AI Foundry project\n",
    "project_conn_str = os.environ.get(\"PROJECT_CONNECTION_STRING\")\n",
    "credential = DefaultAzureCredential()\n",
    "\n",
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=credential,\n",
    "    conn_str=project_conn_str\n",
    ")\n",
    "print(\"‚úÖ Created AIProjectClient.\")\n",
    "\n",
    "# 2) Upload data for evaluation\n",
    "uploaded_data_id, _ = project_client.upload_file(str(eval_data_path))\n",
    "print(\"‚úÖ Uploaded JSONL to project. Data asset ID:\", uploaded_data_id)\n",
    "\n",
    "# 3) Prepare an Azure OpenAI connection for AI-assisted evaluators\n",
    "default_connection = project_client.connections.get_default(\n",
    "    connection_type=ConnectionType.AZURE_OPEN_AI\n",
    ")\n",
    "\n",
    "model_config = default_connection.to_evaluator_model_config(\n",
    "    deployment_name='gpt-4o', \n",
    "    api_version='2024-12-01-preview'\n",
    ")\n",
    "model_config['type'] = 'azure_openai'\n",
    "\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Health Fitness Remote Evaluation\",\n",
    "    description=\"Evaluating dataset for correctness.\",\n",
    "    data=Dataset(id=uploaded_data_id),\n",
    "    evaluators={\n",
    "        \"f1_score\": EvaluatorConfiguration(id=F1ScoreEvaluator.id),\n",
    "        \"relevance\": EvaluatorConfiguration(\n",
    "            id=RelevanceEvaluator.id,\n",
    "            init_params={\"model_config\": model_config},\n",
    "            data_mapping={\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        ),\n",
    "        \"violence\": EvaluatorConfiguration(\n",
    "            id=ViolenceEvaluator.id,\n",
    "            init_params={\"azure_ai_project\": project_client.scope},\n",
    "            data_mapping={\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# Helper: Create evaluation with retry logic\n",
    "def create_evaluation_with_retry(project_client, evaluation, max_retries=3, retry_delay=5):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = project_client.evaluations.create(evaluation=evaluation)\n",
    "            return result\n",
    "        except ServiceResponseError as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            print(f\"‚ö†Ô∏è Attempt {attempt+1} failed: {str(e)}. Retrying in {retry_delay} seconds...\")\n",
    "            time.sleep(retry_delay)\n",
    "\n",
    "# 5) Create & track the evaluation using retry logic\n",
    "cloud_eval = create_evaluation_with_retry(project_client, evaluation)\n",
    "print(\"‚úÖ Created evaluation job. ID:\", cloud_eval.id)\n",
    "\n",
    "# 6) Poll or fetch final status\n",
    "fetched_eval = project_client.evaluations.get(cloud_eval.id)\n",
    "print(\"Current status:\", fetched_eval.status)\n",
    "if hasattr(fetched_eval, 'properties'):\n",
    "    link = fetched_eval.properties.get(\"AiStudioEvaluationUri\", \"\")\n",
    "    if link:\n",
    "        print(\"View details in Foundry:\", link)\n",
    "else:\n",
    "    print(\"No link found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091290cd",
   "metadata": {},
   "source": [
    "### Viewing Cloud Evaluation Results\n",
    "- Navega a la pesta√±a **Evaluations** en tu proyecto AI Foundry para ver tu trabajo de evaluaci√≥n.\n",
    "- Abre la evaluaci√≥n para ver m√©tricas agregadas y detalles a nivel de fila.\n",
    "- Para evaluadores asistidos por IA o de riesgo y seguridad, ver√°s tanto puntajes promedio como resultados detallados por fila."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
