{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3d8f7b1",
      "metadata": {},
      "source": [
        "# üöÄ DeepSeek-R1 Model with Azure AI Inference üß†\n",
        "\n",
        "**DeepSeek-R1** es un modelo de razonamiento de vanguardia que combina aprendizaje por refuerzo y ajuste fino supervisado, destac√°ndose en tareas de razonamiento complejas con 37B par√°metros activos y una ventana de contexto de 128K.\n",
        "\n",
        "En este cuaderno de Jupyter aprender√°s a:\n",
        "1. **Initialize** el ChatCompletionsClient para endpoints sin servidor de Azure\n",
        "2. **Chat** con DeepSeek-R1 utilizando la extracci√≥n de razonamiento\n",
        "3. **Implement** un ejemplo de planificaci√≥n de viajes con razonamiento paso a paso\n",
        "4. **Leverage** la ventana de contexto de 128K para escenarios complejos\n",
        "\n",
        "## ¬øPor qu√© DeepSeek-R1?\n",
        "- **Advanced Reasoning**: Se especializa en la resoluci√≥n de problemas mediante cadenas de pensamiento\n",
        "- **Massive Context**: Ventana de 128K tokens para an√°lisis detallado\n",
        "- **Efficient Architecture**: 37B par√°metros activos de un total de 671B\n",
        "- **Safety Integrated**: Capacidades integradas de filtrado de contenido\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e3a4c2",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "Importar la libreria\n",
        "- **azure-ai-projects**: para el `AIProjectClient`.\n",
        "- **azure-ai-inference**: para llamar los modelos\n",
        "- **azure-identity**: para `DefaultAzureCredential`.\n",
        "\n",
        "Proporciona un archivo `.env` con:\n",
        "```bash\n",
        "PROJECT_CONNECTION_STRING=<your-conn-string>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "a53f8d4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ AIProjectClient created successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage, AssistantMessage\n",
        "import re\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Load environment variables\n",
        "notebook_path = Path().absolute()\n",
        "parent_dir = notebook_path.parent\n",
        "load_dotenv(parent_dir / '../.env', override=True)\n",
        "\n",
        "conn_string = os.getenv(\"PROJECT_CONNECTION_STRING\")\n",
        "model_name = \"DeepSeek-R1\"\n",
        "\n",
        "try:\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(),\n",
        "        conn_str=conn_string,\n",
        "    )\n",
        "    print(\"‚úÖ AIProjectClient created successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error creating AIProjectClient:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c01d5d9",
      "metadata": {},
      "source": [
        "## 2. Intelligent Travel Planning ‚úàÔ∏è\n",
        "\n",
        "Demonstrate DeepSeek-R1's reasoning capabilities for trip planning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e6a5d8d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üó∫Ô∏è Query: Planifica un viaje cultural de 5 d√≠as a Kioto en abril\n",
            "\n",
            "üß† Thinking Process: Okay, the user wants a 5-day cultural trip to Kyoto in April, including hidden gems and safety considerations. Let me start by recalling Kyoto's main attractions and the less-known spots. April is cherry blossom season, so hanami spots are a must, but maybe suggest some quieter ones to avoid crowds.\n",
            "\n",
            "First day: Arrival and central Kyoto. Kiyomizu-dera is a must, but maybe add nearby hidden spots like Sannen-zaka and Ninen-zaka. They‚Äôre historic streets but can get crowded. Safety tip: watch out for slippery stone paths, especially if it rains.\n",
            "\n",
            "Second day: Arashiyama. Bamboo Grove is popular, so suggest going early. Then Okochi Sanso Villa as a hidden gem. Maybe include Gio-ji Temple in the afternoon, which is quieter. Safety in bamboo grove: stay on paths to avoid getting lost.\n",
            "\n",
            "Third day: Northern Kyoto. Ryoan-ji and Ninna-ji are well-known temples. Add Myoman-ji Temple as a hidden gem. Ohara village could be a good half-day trip, less crowded. Safety for hiking trails: wear proper shoes.\n",
            "\n",
            "Fourth day: Fushimi Inari. Early morning to avoid crowds. Tofuku-ji Temple nearby, maybe the Hojo garden. Then Fushimi sake district for a hidden gem. Safety on the mountain trails: carry water and watch steps.\n",
            "\n",
            "Fifth day: Day trip to Uji. Byodo-in Temple is famous, but maybe the Uji River and small tea shops. Also, Mimuroto-ji Temple for hydrangeas, though April might be early. Safety near the river: be cautious if walking along the banks.\n",
            "\n",
            "General safety tips: April weather can be variable, so layers and rain gear. Crowds during cherry blossom, so secure belongings. COVID precautions if still relevant. Transportation: IC cards, maybe bicycle rentals but caution in traffic.\n",
            "\n",
            "Need to balance cultural sites with hidden spots each day. Check if all locations are accessible in the time frame. Maybe include dining options that are local but not too touristy. Also, consider transportation passes for efficiency.\n",
            "\n",
            "Wait, the user mentioned cultural aspects, so tea ceremonies, traditional workshops. Maybe include a tea house in Uji. Also, check if any special events in April, like festivals, but might be too crowded.\n",
            "\n",
            "Make sure each day isn‚Äôt too packed, allowing time to explore. Hidden gems should be interspersed with main attractions to give a well-rounded experience. Safety tips specific to each location, like slippery paths, hiking precautions, and staying hydrated.\n",
            "\n",
            "Double-check the hidden gems: Okochi Sanso, Gio-ji, Myoman-ji, Fushimi sake district, Mimuroto-ji. Are these less touristy? Maybe confirm. Also, consider the opening hours and any entry fees.\n",
            "\n",
            "Transportation: Buses and trains. Maybe recommend using buses for some areas but warn about traffic. Bicycles are good but need to follow traffic rules. Maybe mention taxi options for convenience, though more expensive.\n",
            "\n",
            "Dining: Recommend local specialties each day, like kaiseki, yudofu, matcha dishes in Uji. Suggest specific restaurants if possible, but maybe keep it general unless the user wants specifics.\n",
            "\n",
            "Language tips: Basic phrases, having addresses written in Japanese for taxi drivers. Emergency numbers included.\n",
            "\n",
            "Budget considerations: Mix of free and paid attractions. Some temples have entry fees, so note that. Sake tasting might have a cost.\n",
            "\n",
            "Final check: 5 days, each day with a theme, hidden gems, safety, cultural experiences. Make sure the plan is feasible and not too rushed. Maybe add alternatives in case of bad weather.\n",
            "\n",
            "üìù Final Answer: **Viaje Cultural de 5 D√≠as a Kioto en Abril: Gemas Ocultas y Seguridad**  \n",
            "*Abril es temporada de cerezos en flor (sakura), ideal para combinar cultura, naturaleza y tranquilidad.*\n",
            "\n",
            "---\n",
            "\n",
            "### **D√≠a 1: Llegada y Exploraci√≥n del Centro Hist√≥rico**  \n",
            "**Ma√±ana:**  \n",
            "- **Kiyomizu-dera**: Templo emblem√°tico con vistas panor√°micas. Llega temprano para evitar multitudes.  \n",
            "- **Gemas Ocultas**:  \n",
            "  - **Sannen-zaka y Ninen-zaka**: Calles empedradas con casas de t√© tradicionales. Visita **K≈çdai-ji Temple** (menos concurrido que Kiyomizu).  \n",
            "  - **Entoku-in**: Peque√±o templo cerca de Kiyomizu, famoso por su jard√≠n de musgo.  \n",
            "\n",
            "**Tarde:**  \n",
            "- **Gion**: Paseo por el distrito de geishas. Busca **Shirakawa Minami-dori**, un canal tranquilo rodeado de cerezos.  \n",
            "- **Cena**: Prueba **yudofu** (tofu hervido\n"
          ]
        }
      ],
      "source": [
        "def plan_trip_with_reasoning(query, show_thinking=False):\n",
        "    \"\"\"Get travel recommendations with reasoning extraction\"\"\"\n",
        "    messages = [\n",
        "        SystemMessage(content=\"Eres un experto en viajes. Proporciona planes detallados con justificaci√≥n.\"),\n",
        "        UserMessage(content=f\"{query} Incluye gemas ocultas y consideraciones de seguridad.\")\n",
        "    ]\n",
        "\n",
        "    with project_client.inference.get_chat_completions_client() as chat_client:\n",
        "        response = chat_client.complete(\n",
        "            messages=messages,\n",
        "            model=model_name,\n",
        "            temperature=0.7,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "    \n",
        "    content = response.choices[0].message.content\n",
        "    \n",
        "    # Extract reasoning if present\n",
        "    if show_thinking:\n",
        "        match = re.search(r\"<think>(.*?)</think>(.*)\", content, re.DOTALL)\n",
        "        if match:\n",
        "            return {\"thinking\": match.group(1).strip(), \"answer\": match.group(2).strip()}\n",
        "    return content\n",
        "\n",
        "# Example usage\n",
        "query = \"Planifica un viaje cultural de 5 d√≠as a Kioto en abril\"\n",
        "result = plan_trip_with_reasoning(query, show_thinking=True)\n",
        "\n",
        "print(\"üó∫Ô∏è Query:\", query)\n",
        "if isinstance(result, dict):\n",
        "    print(\"\\nüß† Thinking Process:\", result[\"thinking\"])\n",
        "    print(\"\\nüìù Final Answer:\", result[\"answer\"])\n",
        "else:\n",
        "    print(\"\\nüìù Response:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8f1b3a",
      "metadata": {},
      "source": [
        "## 3. Technical Problem Solving üíª\n",
        "\n",
        "Showcase coding/optimization capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e5d4a3e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Problem: ¬øC√≥mo puedo optimizar una base de datos PostgreSQL que maneja 10k transacciones/segundo?\n",
            "Considera estrategias de indexaci√≥n, requisitos de hardware y optimizaci√≥n de consultas.\n",
            "\n",
            "‚öôÔ∏è Solution: <think>\n",
            "Okay, I need to figure out how to optimize a PostgreSQL database handling 10k transactions per second. That's a high workload, so the solution has to be thorough. Let me start by breaking down the problem into parts: indexing strategies, hardware requirements, and query optimization. \n",
            "\n",
            "First, indexing. I know that proper indexing is crucial for performance. But using too many indexes can slow down writes. So maybe I should look at the most frequent queries and see which columns are used in WHERE clauses, JOINs, or ORDER BY. Maybe using B-tree indexes for common lookups. Also, composite indexes if queries filter on multiple columns. But wait, the order of columns in composite indexes matters. They should be ordered from the most specific to the least specific. What about indexes on foreign keys? If there are a lot of joins, that could help. Also, maybe partial indexes if there are queries that frequently filter on a specific value. For example, if a status column often has a value like 'active', a partial index on WHERE status = 'active' could be smaller and faster.\n",
            "\n",
            "Then there's the issue of index bloat. Over time, indexes can become fragmented, especially with high write volumes. So regular maintenance with REINDEX or VACUUM might be necessary. Oh right, PostgreSQL has autovacuum, but maybe tuning its parameters for such a high transaction rate. Increasing autovacuum workers or lowering the thresholds so it runs more frequently to prevent bloat.\n",
            "\n",
            "Next, hardware considerations. 10k transactions per second is a lot. The storage needs to be fast. SSDs or NVMe drives would be essential to handle the IOPS. RAID configurations for redundancy and performance. Maybe RAID 10. Also, enough RAM to fit the working set. The shared_buffers parameter in PostgreSQL should be set to a portion of the available RAM, typically around 25% of total RAM. But if the server has a lot of RAM, maybe up to 40%, but not more than that. Also, the OS should have enough memory for disk caching. \n",
            "\n",
            "CPU cores are important too. PostgreSQL is good at parallelizing queries, so having multiple cores can help. If the workload is read-heavy, maybe using read replicas to distribute the load. But if it's write-heavy, scaling writes is trickier. Maybe connection pooling to handle the high number of connections efficiently, like using pgbouncer. Because creating a new connection for each transaction is expensive. \n",
            "\n",
            "Storage configuration: Using a separate disk for the transaction logs (WAL) to reduce contention. Also, adjusting the checkpoint parameters. Checkpoints can be a bottleneck if not tuned. Increasing checkpoint_segments and checkpoint_timeout to reduce the frequency of checkpoints, but that would require more disk space for WAL. Alternatively, using a higher checkpoint_completion_target to spread out the disk I/O.\n",
            "\n",
            "Now, query optimization. Analyzing slow queries with EXPLAIN and EXPLAIN ANALYZE to find bottlenecks. Maybe rewriting queries to be more efficient, avoiding SELECT *, using joins instead of subqueries where possible. Ensuring that the queries are using indexes properly. Also, avoiding unnecessary locks by keeping transactions short. Maybe breaking up large transactions into smaller ones to reduce lock contention. \n",
            "\n",
            "Caching frequently accessed data. PostgreSQL has a built-in cache, but maybe using an external cache like Redis for hot data. Application-level caching could reduce the load on the database. Also, materialized views for complex queries that don't need real-time data, which can be refreshed periodically instead of computing on the fly.\n",
            "\n",
            "Partitioning large tables. If certain tables are huge, partitioning them by range or list can make queries faster by allowing the database to scan only relevant partitions. For example, partitioning by date if the data is time-series. PostgreSQL supports declarative partitioning, which can help manage this.\n",
            "\n",
            "Connection management. Using a connection pooler to avoid the overhead of establishing new connections for each transaction. As mentioned earlier, pgbouncer in transaction pooling mode can help reuse connections efficiently. Also, configuring the max_connections parameter appropriately, but not too high as each connection consumes resources.\n",
            "\n",
            "Replication and load balancing. If reads are a significant portion, setting up streaming replication with read replicas to distribute read queries. Using a load balancer to direct traffic to the replicas. For write scalability, maybe looking into sharding, though that's more complex and application-dependent. Tools like Citus could help with sharding in PostgreSQL.\n",
            "\n",
            "Configuration tuning. Adjusting parameters like work_mem for sorting and joins, effective_cache_size to let the planner know how much memory is available. Also, random_page_cost if using SSDs, which should be lower than the default (which is set for HDDs). Synchronous_commit can be turned off for faster writes, but that risks losing some data in case of a crash. Maybe using a battery-backed write cache on the RAID controller to mitigate that.\n",
            "\n",
            "Monitoring and logging. Using tools like pg_stat_statements to track query performance over time. Logging slow queries to identify candidates for optimization. Setting up alerts for long transactions or lock waits.\n",
            "\n",
            "Wait, I should also consider the use of prepared statements to reduce parsing overhead. And maybe using stored procedures for frequently executed transactions to minimize network round trips.\n",
            "\n",
            "Let me check if I missed anything. Indexing strategies: right, covering indexes with INCLUDE clause to include columns that are often selected, avoiding heap lookups. Hardware: CPU, RAM, fast storage, network latency. Query optimization: avoiding N+1 queries, using batch operations instead of individual inserts/updates. Also, transaction isolation levels. Using the appropriate level to minimize locks. For example, using READ COMMITTED instead of SERIALIZABLE if possible.\n",
            "\n",
            "Another thing: table statistics. Making sure that autovacuum is properly configured so that the query planner has accurate statistics. If the stats are outdated, the planner might choose bad execution plans. So tuning autovacuum_analyze_scale_factor and autovacuum_analyze_threshold for heavily updated tables.\n",
            "\n",
            "Also, considering the use of UNLOGGED tables for data that can be regenerated, to reduce WAL writes. But that's risky because they get truncated on crash recovery.\n",
            "\n",
            "Hmm, maybe using connection poolers with transaction pooling to handle the high TPS. Also, application-side optimizations like batching transactions if possible.\n",
            "\n",
            "Putting it all together, the answer should cover these areas with specific recommendations. Let me structure them step by step.\n",
            "</think>\n",
            "\n",
            "Para optimizar una base de datos PostgreSQL que maneja 10k transacciones/segundo, se deben abordar tres √°reas clave: **indexaci√≥n**, **hardware** y **optimizaci√≥n de consultas**. A continuaci√≥n, se detallan las estrategias:\n",
            "\n",
            "### 1. **Estrategias de Indexaci√≥n**\n",
            "- **√çndices B-tree**: Para columnas frecuentes en cl√°usulas `WHERE`, `JOIN` y `ORDER BY`.\n",
            "- **√çndices compuestos**: Ordenar columnas por selectividad (m√°s espec√≠ficas primero).\n",
            "- **√çndices parciales**: En filtros comunes (ej: `WHERE status = 'active'`).\n",
            "- **√çndices covering**: Usar `INCLUDE` para columnas accedidas frecuentemente, evitando accesos a la tabla.\n",
            "- **Mantenimiento de √≠ndices**: Ejecutar `REINDEX` y ajustar `autovacuum` (aumentar `autovacuum_vacuum_scale_factor` y reducir `autovacuum_vacuum_cost_limit` para evitar *bloat*).\n",
            "\n",
            "### 2. **Requisitos de Hardware**\n",
            "- **Almacenamiento r√°pido**: NVMe/SSD con RAID 10 para alto IOPS y redundancia.\n",
            "- **RAM suficiente**: M√≠nimo 64-128 GB, asignando 25-40% a `shared_buffers`.\n",
            "- **CPU multicore**: Procesadores de alto rendimiento (ej: 16+ n√∫cleos) para paralelizar consultas.\n",
            "- **Redes de baja latencia**: Evitar cuellos de botella en comunicaci√≥n.\n",
            "- **Separaci√≥n de WAL**: Almacenar logs de transacciones en discos dedicados.\n",
            "\n",
            "### 3. **Optimizaci√≥n de Consultas**\n",
            "- **An√°lisis con `EXPLAIN ANALYZE`**: Identificar consultas lentas y optimizar planes de ejecuci√≥n.\n",
            "- **Evitar `SELECT *`**: Seleccionar solo columnas necesarias.\n",
            "- **Batch operations**: Agrupar inserciones/actualizaciones para reducir overhead.\n",
            "- **Transacciones cortas**: Minimizar bloqueos y tiempos de espera.\n",
            "- **Pool de conexiones**: Usar `pgbouncer` en modo *transaction pooling* para reducir conexiones activas.\n",
            "- **Materialized views**: Para consultas complejas y est√°ticas, actualizadas peri√≥dicamente.\n",
            "\n",
            "### 4. **Configuraci√≥n de PostgreSQL**\n",
            "- **Ajuste de par√°metros**:\n",
            "  - `work_mem`: Aumentar (ej: 16-64MB) para operaciones de ordenamiento.\n",
            "  - `random_page_cost`: Reducir a ~1.1 si se usan SSDs.\n",
            "  - `checkpoint_completion_target`: 0.9 para distribuir escrituras de WAL.\n",
            "  - `max_connections`: Limitar y usar *poolers* para evitar sobrecarga.\n",
            "- **Replicaci√≥n y escalado**: R√©plicas de lectura para distribuir carga y herramientas como Citus para *sharding*.\n",
            "\n",
            "### 5. **Monitoreo y Mantenimiento**\n",
            "- **Herramientas**: `pg_stat_statements`, `pgBadger` y alertas para *locks* largos o transacciones lentas.\n",
            "- **Autovacuum tuning**: Ajust\n"
          ]
        }
      ],
      "source": [
        "def solve_technical_problem(problem):\n",
        "    \"\"\"Solve complex technical problems with structured reasoning\"\"\"\n",
        "    with project_client.inference.get_chat_completions_client() as chat_client:\n",
        "        response = chat_client.complete(\n",
        "            messages=[\n",
        "            UserMessage(content=f\"{problem} Por favor razona paso a paso, y coloca tu respuesta final dentro de \\\\boxed{{}}.\")\n",
        "            ],\n",
        "            model=model_name,\n",
        "            temperature=0.3,\n",
        "            max_tokens=2048\n",
        "        )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Database optimization example\n",
        "problem = \"\"\"¬øC√≥mo puedo optimizar una base de datos PostgreSQL que maneja 10k transacciones/segundo?\n",
        "Considera estrategias de indexaci√≥n, requisitos de hardware y optimizaci√≥n de consultas.\"\"\"\n",
        "\n",
        "print(\"üîß Problem:\", problem)\n",
        "print(\"\\n‚öôÔ∏è Solution:\", solve_technical_problem(problem))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
