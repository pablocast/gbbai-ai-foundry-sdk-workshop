{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3d8f7b1",
      "metadata": {},
      "source": [
        "# üöÄ DeepSeek-R1 Model with Azure AI Inference üß†\n",
        "\n",
        "**DeepSeek-R1** es un modelo de razonamiento de vanguardia que combina aprendizaje por refuerzo y ajuste fino supervisado, destac√°ndose en tareas de razonamiento complejas con 37B par√°metros activos y una ventana de contexto de 128K.\n",
        "\n",
        "En este cuaderno de Jupyter aprender√°s a:\n",
        "1. **Initialize** el ChatCompletionsClient para endpoints sin servidor de Azure\n",
        "2. **Chat** con DeepSeek-R1 utilizando la extracci√≥n de razonamiento\n",
        "3. **Implement** un ejemplo de planificaci√≥n de viajes con razonamiento paso a paso\n",
        "4. **Leverage** la ventana de contexto de 128K para escenarios complejos\n",
        "\n",
        "## ¬øPor qu√© DeepSeek-R1?\n",
        "- **Advanced Reasoning**: Se especializa en la resoluci√≥n de problemas mediante cadenas de pensamiento\n",
        "- **Massive Context**: Ventana de 128K tokens para an√°lisis detallado\n",
        "- **Efficient Architecture**: 37B par√°metros activos de un total de 671B\n",
        "- **Safety Integrated**: Capacidades integradas de filtrado de contenido\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e3a4c2",
      "metadata": {},
      "source": [
        "## 1. Setup & Authentication\n",
        "\n",
        "Requerimientos:\n",
        "- `azure-ai-inference`: Para chat completions\n",
        "- `python-dotenv`: Para las variables de ambiente\n",
        "\n",
        "Requerimientos del archivo .env \n",
        "```bash\n",
        "AZURE_INFERENCE_ENDPOINT=<your-endpoint-url>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53f8d4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Initialization failed: (None) Unauthorized. Access token is missing, invalid, audience is incorrect (https://cognitiveservices.azure.com), or have expired.\n",
            "Code: None\n",
            "Message: Unauthorized. Access token is missing, invalid, audience is incorrect (https://cognitiveservices.azure.com), or have expired.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from dotenv import load_dotenv\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Load environment variables\n",
        "notebook_path = Path().absolute()\n",
        "parent_dir = notebook_path.parent\n",
        "load_dotenv(parent_dir / '../.env')\n",
        "\n",
        "endpoint = os.getenv(\"AZURE_INFERENCE_ENDPOINT\")\n",
        "model_name = \"DeepSeek-R1\"\n",
        "key = os.getenv(\"AZURE_INFERENCE_KEY\")\n",
        "\n",
        "# Initialize client\n",
        "try:\n",
        "    client = ChatCompletionsClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(key)\n",
        "    )\n",
        "    print(\"‚úÖ Client initialized | Model:\", client.get_model_info().model_name)\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Initialization failed:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c01d5d9",
      "metadata": {},
      "source": [
        "## 2. Intelligent Travel Planning ‚úàÔ∏è\n",
        "\n",
        "Demonstrate DeepSeek-R1's reasoning capabilities for trip planning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6a5d8d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "def plan_trip_with_reasoning(query, show_thinking=False):\n",
        "    \"\"\"Get travel recommendations with reasoning extraction\"\"\"\n",
        "    messages = [\n",
        "        SystemMessage(content=\"You are a travel expert. Provide detailed plans with rationale.\"),\n",
        "        UserMessage(content=f\"{query} Include hidden gems and safety considerations.\")\n",
        "    ]\n",
        "    \n",
        "    response = client.complete(\n",
        "        messages=messages,\n",
        "        model=model_name,\n",
        "        temperature=0.7,\n",
        "        max_tokens=1024\n",
        "    )\n",
        "    \n",
        "    content = response.choices[0].message.content\n",
        "    \n",
        "    # Extract reasoning if present\n",
        "    if show_thinking:\n",
        "        match = re.search(r\"<think>(.*?)</think>(.*)\", content, re.DOTALL)\n",
        "        if match:\n",
        "            return {\"thinking\": match.group(1).strip(), \"answer\": match.group(2).strip()}\n",
        "    return content\n",
        "\n",
        "# Example usage\n",
        "query = \"Plan a 5-day cultural trip to Kyoto in April\"\n",
        "result = plan_trip_with_reasoning(query, show_thinking=True)\n",
        "\n",
        "print(\"üó∫Ô∏è Query:\", query)\n",
        "if isinstance(result, dict):\n",
        "    print(\"\\nüß† Thinking Process:\", result[\"thinking\"])\n",
        "    print(\"\\nüìù Final Answer:\", result[\"answer\"])\n",
        "else:\n",
        "    print(\"\\nüìù Response:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8f1b3a",
      "metadata": {},
      "source": [
        "## 3. Technical Problem Solving üíª\n",
        "\n",
        "Showcase coding/optimization capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5d4a3e1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def solve_technical_problem(problem):\n",
        "    \"\"\"Solve complex technical problems with structured reasoning\"\"\"\n",
        "    response = client.complete(\n",
        "        messages=[\n",
        "            UserMessage(content=f\"{problem} Please reason step by step, and put your final answer within \\boxed{{}}.\")\n",
        "        ],\n",
        "        model=model_name,\n",
        "        temperature=0.3,\n",
        "        max_tokens=2048\n",
        "    )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Database optimization example\n",
        "problem = \"\"\"How can I optimize a PostgreSQL database handling 10k transactions/second?\n",
        "Consider indexing strategies, hardware requirements, and query optimization.\"\"\"\n",
        "\n",
        "print(\"üîß Problem:\", problem)\n",
        "print(\"\\n‚öôÔ∏è Solution:\", solve_technical_problem(problem))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9f7a8c",
      "metadata": {},
      "source": [
        "## 4. Best Practices & Considerations\n",
        "\n",
        "1. **Reasoning Handling**: Use regex to separate <think> content from final answers\n",
        "2. **Safety**: Built-in content filtering - handle HttpResponseError for violations\n",
        "3. **Performance**:\n",
        "   - Max tokens: 4096\n",
        "   - Rate limit: 200K tokens/minute\n",
        "4. **Cost**: Pay-as-you-go with serverless deployment\n",
        "5. **Streaming**: Implement response streaming for long completions\n",
        "\n",
        "```python\n",
        "# Streaming example\n",
        "response = client.complete(..., stream=True)\n",
        "for chunk in response:\n",
        "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
        "```\n",
        "\n",
        "## üéØ Key Takeaways\n",
        "- Leverage 128K context for detailed analysis\n",
        "- Extract reasoning steps for debugging/analysis\n",
        "- Combine with Azure AI Content Safety for production\n",
        "- Monitor token usage via response.usage\n",
        "\n",
        "> Always validate model outputs for critical applications!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
