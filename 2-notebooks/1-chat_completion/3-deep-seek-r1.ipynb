{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a3d8f7b1",
      "metadata": {},
      "source": [
        "# üöÄ DeepSeek-R1 Model with Azure AI Inference üß†\n",
        "\n",
        "**DeepSeek-R1** es un modelo de razonamiento de vanguardia que combina aprendizaje por refuerzo y ajuste fino supervisado, destac√°ndose en tareas de razonamiento complejas con 37B par√°metros activos y una ventana de contexto de 128K.\n",
        "\n",
        "En este cuaderno de Jupyter aprender√°s a:\n",
        "1. **Initialize** el ChatCompletionsClient para endpoints sin servidor de Azure\n",
        "2. **Chat** con DeepSeek-R1 utilizando la extracci√≥n de razonamiento\n",
        "3. **Implement** un ejemplo de planificaci√≥n de viajes con razonamiento paso a paso\n",
        "4. **Leverage** la ventana de contexto de 128K para escenarios complejos\n",
        "\n",
        "## ¬øPor qu√© DeepSeek-R1?\n",
        "- **Advanced Reasoning**: Se especializa en la resoluci√≥n de problemas mediante cadenas de pensamiento\n",
        "- **Massive Context**: Ventana de 128K tokens para an√°lisis detallado\n",
        "- **Efficient Architecture**: 37B par√°metros activos de un total de 671B\n",
        "- **Safety Integrated**: Capacidades integradas de filtrado de contenido\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e3a4c2",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "Importar la libreria\n",
        "- **azure-ai-projects**: para el `AIProjectClient`.\n",
        "- **azure-ai-inference**: para llamar los modelos\n",
        "- **azure-identity**: para `DefaultAzureCredential`.\n",
        "\n",
        "Proporciona un archivo `.env` con:\n",
        "```bash\n",
        "PROJECT_CONNECTION_STRING=<your-conn-string>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a53f8d4c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ AIProjectClient created successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from pathlib import Path\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.projects import AIProjectClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage, AssistantMessage\n",
        "import re\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Load environment variables\n",
        "notebook_path = Path().absolute()\n",
        "parent_dir = notebook_path.parent\n",
        "load_dotenv(parent_dir / '../.env', override=True)\n",
        "\n",
        "conn_string = os.getenv(\"PROJECT_CONNECTION_STRING\")\n",
        "model_name = \"DeepSeek-R1\"\n",
        "\n",
        "try:\n",
        "    project_client = AIProjectClient.from_connection_string(\n",
        "        credential=DefaultAzureCredential(),\n",
        "        conn_str=conn_string,\n",
        "    )\n",
        "    print(\"‚úÖ AIProjectClient created successfully!\")\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Error creating AIProjectClient:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c01d5d9",
      "metadata": {},
      "source": [
        "## 2. Intelligent Travel Planning ‚úàÔ∏è\n",
        "\n",
        "Demonstrate DeepSeek-R1's reasoning capabilities for trip planning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e6a5d8d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üó∫Ô∏è Query: Planifica un viaje cultural de 5 d√≠as a Kioto en abril\n",
            "\n",
            "üß† Thinking Process: Okay, the user wants a 5-day cultural trip to Kyoto in April, including hidden gems and safety considerations. Let me start by recalling Kyoto's main attractions and the less-known spots. April is cherry blossom season, so hanami spots are a must, but maybe suggest some quieter ones to avoid crowds.\n",
            "\n",
            "First day: Arrival and Gion. Kiyomizu-dera is a must, but maybe add nearby Sannenzaka and Ninenzaka streets. For a hidden gem, maybe the Ishibei-koji alley. Safety tip: crowded areas mean pickpocketing risk, so advise secure bags.\n",
            "\n",
            "Second day: Arashiyama. Bamboo Grove is popular, but early morning visit to avoid crowds. Okochi Sanso Villa is a nice hidden spot. Then maybe Ryoan-ji for the rock garden. Safety: watch for slippery paths in temples.\n",
            "\n",
            "Third day: Northern Kyoto. Kurama and Kibune are less touristy. Kurama-dera hike, then Kibune Shrine. Lunch at a riverside restaurant. Afternoon at Shimogamo Shrine. Safety: hiking shoes for the trail, check weather.\n",
            "\n",
            "Fourth day: Fushimi Inari. Early morning to beat crowds. Hidden gem Tofuku-ji Temple. Afternoon in Uji for tea culture. Evening Pontocho Alley. Safety: stay hydrated, be cautious at night in Pontocho.\n",
            "\n",
            "Fifth day: Nijo Castle and Nishiki Market. Maybe add a hidden gem like Sento Imperial Palace. Departure with last-minute shopping.\n",
            "\n",
            "Also, general tips: transport with IC cards, respect etiquette, weather in April is mild but rain possible. COVID measures might still be in place, so check guidelines. Include emergency numbers and note about Golden Week if late April.\n",
            "\n",
            "Make sure each day balances popular spots with hidden gems, includes practical safety tips, and cultural insights. Check if all locations are logically ordered to minimize travel time. Maybe suggest specific restaurants or tea houses for authentic experiences. Ensure the plan is feasible within 5 days without being too rushed.\n",
            "\n",
            "üìù Final Answer: **Viaje Cultural de 5 D√≠as a Kioto en Abril: Gemas Ocultas y Seguridad**  \n",
            "Kioto, cuna de la cultura tradicional japonesa, combina templos milenarios, jardines serenos y festivales de primavera. Abril es ideal por el *sakura* (flores de cerezo) y el clima templado. Aqu√≠ tienes un plan detallado:\n",
            "\n",
            "---\n",
            "\n",
            "### **D√≠a 1: Llegada y Exploraci√≥n del Distrito de Gion**  \n",
            "**Ma√±ana:**  \n",
            "- **Kiyomizu-dera**: Templo emblem√°tico con vistas panor√°micas. Llega temprano para evitar multitudes.  \n",
            "- **Gema oculta**: **Ishibei-koji**, un callej√≥n empedrado cerca de Kodai-ji, con casas de t√© tradicionales y menos turistas.  \n",
            "\n",
            "**Tarde:**  \n",
            "- **Sannenzaka y Ninenzaka**: Calles hist√≥ricas con tiendas de artesan√≠a. Prueba *matcha* en un sal√≥n de t√©.  \n",
            "- **Gion Corner**: Espect√°culo cultural nocturno con geishas (reserva con antelaci√≥n).  \n",
            "\n",
            "**Seguridad**:  \n",
            "- Cuidado con el pavimento irregular en zonas hist√≥ricas.  \n",
            "- Evita fotografiar geishas sin permiso (considerado irrespetuoso).  \n",
            "\n",
            "---\n",
            "\n",
            "### **D√≠a 2: Arashiyama y Templos Zen**  \n",
            "**Ma√±ana:**  \n",
            "- **Bosque de Bamb√∫ de Arashiyama**: Visita antes de las 8:00 am para disfrutarlo en soledad.  \n",
            "- **Gema oculta**: **Okochi Sanso Villa**, jardines privados con t√© incluido (¬•1,000).  \n",
            "\n",
            "**Tarde:**  \n",
            "- **Templo Tenryu-ji**: Patrimonio de la UNESCO con un jard√≠n zen.  \n",
            "- **Gema oculta**: **Adashino Nenbutsu-ji**, un cementerio con miles de estatuas budistas (menos concurrido que otros templos).  \n",
            "\n",
            "**Seguridad**:  \n",
            "- Respeta el silencio en templos y jardines zen.  \n",
            "- Usa calzado antideslizante (suelos de piedra h√∫medos).  \n",
            "\n",
            "---\n",
            "\n",
            "### **D√≠a 3: Norte de Kioto y Naturaleza**  \n",
            "**Ma√±ana:**  \n",
            "- **Kurama-dera**: Templo en la monta√±a accesible por un sendero boscoso (45 min desde la estaci√≥n de Kurama).  \n",
            "- **Gema oculta**: **Kibune**, un pueblo cercano con el santuario Kibune-jinja y restaurantes sobre el r√≠o (p. ej\n"
          ]
        }
      ],
      "source": [
        "def plan_trip_with_reasoning(query, show_thinking=False):\n",
        "    \"\"\"Get travel recommendations with reasoning extraction\"\"\"\n",
        "    messages = [\n",
        "        SystemMessage(content=\"Eres un experto en viajes. Proporciona planes detallados con justificaci√≥n.\"),\n",
        "        UserMessage(content=f\"{query} Incluye gemas ocultas y consideraciones de seguridad.\")\n",
        "    ]\n",
        "\n",
        "    with project_client.inference.get_chat_completions_client() as chat_client:\n",
        "        response = chat_client.complete(\n",
        "            messages=messages,\n",
        "            model=model_name,\n",
        "            temperature=0.7,\n",
        "            max_tokens=1024\n",
        "        )\n",
        "    \n",
        "    content = response.choices[0].message.content\n",
        "    \n",
        "    # Extract reasoning if present\n",
        "    if show_thinking:\n",
        "        match = re.search(r\"<think>(.*?)</think>(.*)\", content, re.DOTALL)\n",
        "        if match:\n",
        "            return {\"thinking\": match.group(1).strip(), \"answer\": match.group(2).strip()}\n",
        "    return content\n",
        "\n",
        "# Example usage\n",
        "query = \"Planifica un viaje cultural de 5 d√≠as a Kioto en abril\"\n",
        "result = plan_trip_with_reasoning(query, show_thinking=True)\n",
        "\n",
        "print(\"üó∫Ô∏è Query:\", query)\n",
        "if isinstance(result, dict):\n",
        "    print(\"\\nüß† Thinking Process:\", result[\"thinking\"])\n",
        "    print(\"\\nüìù Final Answer:\", result[\"answer\"])\n",
        "else:\n",
        "    print(\"\\nüìù Response:\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8f1b3a",
      "metadata": {},
      "source": [
        "## 3. Technical Problem Solving üíª\n",
        "\n",
        "Showcase coding/optimization capabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e5d4a3e1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Problem: ¬øC√≥mo puedo optimizar una base de datos PostgreSQL que maneja 10k transacciones/segundo?\n",
            "Considera estrategias de indexaci√≥n, requisitos de hardware y optimizaci√≥n de consultas.\n",
            "\n",
            "‚öôÔ∏è Solution: <think>\n",
            "Okay, I need to figure out how to optimize a PostgreSQL database handling 10k transactions per second. That's a high workload, so the solution has to be thorough. Let me start by breaking down the problem into parts: indexing strategies, hardware requirements, and query optimization. \n",
            "\n",
            "First, indexing. I remember that proper indexing is crucial for query performance. But using too many indexes can slow down writes. So, which indexes are most effective here? Maybe B-tree for common query conditions? Also, maybe considering composite indexes if queries filter on multiple columns. What about covering indexes to include all needed columns and avoid table scans? Oh, and partial indexes for frequently queried subsets of data. Index bloat could be an issue with high transactions, so regular maintenance like REINDEX or VACUUM might be necessary. Wait, but VACUUM is more about transaction ID wraparound and dead tuples. Right, so autovacuum settings might need tuning to keep up with the high transaction rate.\n",
            "\n",
            "Next, hardware. High TPS means the database needs fast disks, enough RAM, and sufficient CPU. SSDs or NVMe drives would be better than HDDs for I/O. RAM should be enough to hold the working set, so the buffer hit rate is high. The shared_buffers parameter in PostgreSQL should be set to a portion of the total RAM, maybe 25% as a starting point, but might need adjustment. Also, checkpoints and WAL settings. If checkpoints are happening too often, they can cause I/O spikes. Increasing checkpoint_timeout and tuning checkpoint_completion_target could spread out the I/O load. For CPU, having multiple cores helps since PostgreSQL can parallelize some operations. Maybe using connection pooling to handle the 10k connections without overloading the system, since each connection has overhead. Pgbouncer or similar tools can help manage that.\n",
            "\n",
            "Query optimization. Slow queries can bottleneck the system. Using EXPLAIN ANALYZE to find queries with high execution times. Maybe rewriting queries to be more efficient, avoiding SELECT *, using joins properly, and ensuring that the queries are using indexes. Also, optimizing transactions by keeping them as short as possible to reduce lock contention. Maybe using batch operations for inserts/updates instead of individual statements. Application-level caching could reduce the load on the database. Materialized views for complex queries that don't need real-time data. Partitioning large tables to divide them into smaller chunks, which can improve both query and maintenance operations.\n",
            "\n",
            "Wait, partitioning. If the data is time-based or has a natural key, partitioning can help. PostgreSQL supports declarative partitioning. That could reduce the index size per partition and make queries faster by pruning partitions. Also, connection pooling is important. If the app is opening too many connections, that can cause resource contention. Using a pooler like pgbouncer in transaction mode can help reuse connections efficiently.\n",
            "\n",
            "Replication and read replicas. If there's a high read load, offloading reads to replicas might help. But the question mentions transactions, which are likely writes. So maybe read replicas aren't the main focus here, but if some transactions are read-only, that could help. For write-heavy workloads, ensuring the primary server is optimized for writes.\n",
            "\n",
            "Configuration tuning. The postgresql.conf settings need adjustments. shared_buffers, work_mem, maintenance_work_mem, max_connections (but with connection pooling, maybe keep max_connections lower and let the pooler handle it). effective_cache_size should reflect the available RAM. Also, adjusting wal_buffers and checkpoint settings. Enabling synchronous_commit might be necessary for data integrity, but if some transactions can tolerate async commits, that could reduce latency. However, with 10k TPS, maybe synchronous_commit should stay on, but tune other WAL parameters.\n",
            "\n",
            "Monitoring and profiling. Tools like pg_stat_statements to identify slow queries. Monitoring tools to track performance metrics: CPU, disk I/O, memory usage, lock contention. Logging slow queries and analyzing them regularly.\n",
            "\n",
            "What about connection settings? Idle in transaction connections can hold locks, so ensuring the application doesn't leave transactions open. Setting a statement_timeout or idle_in_transaction_session_timeout to prevent long-running queries from hogging resources.\n",
            "\n",
            "Also, considering the use of UNLOGGED tables for temporary data that can be recreated, which reduces WAL overhead. But that's risky because they're not crash-safe.\n",
            "\n",
            "Hardware redundancy: RAID configurations for disks, possibly using RAID 10 for performance and redundancy. Battery-backed write cache on the RAID controller if using one. Network latency between app and DB should be low, maybe co-located in the same data center.\n",
            "\n",
            "Wait, but the question is about optimizing the database itself. So maybe the hardware part is about the server specs. Enough CPU cores, fast storage, sufficient RAM. Let's say, for 10k TPS, you need a server with multiple cores (like 16+), 64GB+ RAM, NVMe storage. But exact numbers depend on the workload.\n",
            "\n",
            "Another thing: transaction isolation levels. Using the appropriate level to balance consistency and performance. For example, using read committed instead of serializable if possible.\n",
            "\n",
            "Also, maybe using prepared statements to reduce parsing overhead. But that's more about application-side optimization.\n",
            "\n",
            "Putting it all together, the steps would involve:\n",
            "\n",
            "1. Analyze and optimize queries, use indexes effectively.\n",
            "2. Tune PostgreSQL configuration parameters for high concurrency and throughput.\n",
            "3. Ensure hardware can handle the load with fast storage, enough RAM, and CPUs.\n",
            "4. Implement connection pooling to manage the number of connections.\n",
            "5. Regular maintenance tasks like vacuum, analyze, and reindexing.\n",
            "6. Partitioning large tables and using appropriate storage settings.\n",
            "7. Monitoring and continuous profiling to identify bottlenecks.\n",
            "\n",
            "I need to make sure all these points are covered in the answer, organized into the three main areas: indexing, hardware, query optimization, and maybe some additional considerations like configuration and maintenance.\n",
            "</think>\n",
            "\n",
            "Para optimizar una base de datos PostgreSQL que maneja 10k transacciones/segundo, se deben abordar estrategias de indexaci√≥n, hardware y optimizaci√≥n de consultas de manera integral:\n",
            "\n",
            "### 1. **Estrategias de Indexaci√≥n**\n",
            "- **√çndices B-tree**: √ötiles para consultas con filtros en columnas frecuentes (ej. claves primarias, campos de tiempo).\n",
            "- **√çndices compuestos**: Si las consultas filtran por m√∫ltiples columnas, crear √≠ndices en combinaciones relevantes.\n",
            "- **√çndices de cobertura (INCLUDE)**: Incluir columnas adicionales para evitar accesos a la tabla.\n",
            "- **√çndices parciales**: Si ciertos subconjuntos de datos (ej. `WHERE status = 'active'`) son consultados frecuentemente.\n",
            "- **Mantenimiento de √≠ndices**: Ejecutar `REINDEX` o `VACUUM ANALYZE` regularmente para reducir el *bloat*. Ajustar `autovacuum` para alta carga:\n",
            "  ```sql\n",
            "  autovacuum_max_workers = 6\n",
            "  autovacuum_vacuum_scale_factor = 0.1\n",
            "  ```\n",
            "\n",
            "### 2. **Requisitos de Hardware**\n",
            "- **Almacenamiento r√°pido**: Usar SSD/NVMe para reducir latencia de I/O, especialmente en operaciones de WAL y checkpoints.\n",
            "- **RAM suficiente**: Asignar ~75% de la RAM al sistema operativo y cach√©s. Ajustar `shared_buffers` (25-40% de la RAM total) y `effective_cache_size`.\n",
            "- **CPU multicore**: PostgreSQL aprovecha paralelismo; usar CPUs con 16+ n√∫cleos.\n",
            "- **Red y redundancia**: RAID 10 para equilibrio entre rendimiento y redundancia. Red de baja latencia entre aplicaci√≥n y base de datos.\n",
            "\n",
            "### 3. **Optimizaci√≥n de Consultas**\n",
            "- **An√°lisis de consultas**: Usar `EXPLAIN ANALYZE` y `pg_stat_statements` para identificar cuellos de botella.\n",
            "- **Evitar consultas costosas**: Minimizar `SELECT *`, usar paginaci√≥n eficiente, y reducir subconsultas innecesarias.\n",
            "- **Transacciones cortas**: Reducir tiempos de bloqueo y contenci√≥n. Usar batch processing para inserciones/actualizaciones masivas.\n",
            "- **Particionamiento**: Dividir tablas grandes por rangos (ej. fechas) para mejorar gesti√≥n y rendimiento.\n",
            "- **Pool de conexiones**: Usar herramientas como PgBouncer para limitar conexiones activas y reducir sobrecarga.\n",
            "\n",
            "### 4. **Ajustes de Configuraci√≥n**\n",
            "- **Checkpoints**: \n",
            "  ```sql\n",
            "  checkpoint_timeout = 30min\n",
            "  checkpoint_completion_target = 0.9\n",
            "  ```\n",
            "- **WAL**: \n",
            "  ```sql\n",
            "  wal_buffers = 16MB\n",
            "  max_wal_size = 32GB\n",
            "  ```\n",
            "- **Memoria**: \n",
            "  ```sql\n",
            "  work_mem = 8MB  # Ajustar seg√∫n carga\n",
            "  maintenance_work_mem = 2GB\n",
            "  ```\n",
            "- **Concurrencia**: \n",
            "  ```sql\n",
            "  max_connections = 300  # + Pool externo\n",
            "  ```\n",
            "\n",
            "### 5. **Estrategias Adicionales**\n",
            "- **R√©plicas de lectura**: Descargar consultas SELECT a r√©plicas si el workload es mixto.\n",
            "- **Cach√© de aplicaci√≥n**: Usar Redis/Memcached para datos frecuentes y reducir carga en PostgreSQL.\n",
            "- **Logging y monitoreo**: Herramientas como Prometheus + Grafana para m√©tricas en tiempo real. Activar `log_min_duration_st\n"
          ]
        }
      ],
      "source": [
        "def solve_technical_problem(problem):\n",
        "    \"\"\"Solve complex technical problems with structured reasoning\"\"\"\n",
        "    with project_client.inference.get_chat_completions_client() as chat_client:\n",
        "        response = chat_client.complete(\n",
        "            messages=[\n",
        "            UserMessage(content=f\"{problem} Por favor razona paso a paso, y coloca tu respuesta final dentro de \\\\boxed{{}}.\")\n",
        "            ],\n",
        "            model=model_name,\n",
        "            temperature=0.3,\n",
        "            max_tokens=2048\n",
        "        )\n",
        "    \n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# Database optimization example\n",
        "problem = \"\"\"¬øC√≥mo puedo optimizar una base de datos PostgreSQL que maneja 10k transacciones/segundo?\n",
        "Considera estrategias de indexaci√≥n, requisitos de hardware y optimizaci√≥n de consultas.\"\"\"\n",
        "\n",
        "print(\"üîß Problem:\", problem)\n",
        "print(\"\\n‚öôÔ∏è Solution:\", solve_technical_problem(problem))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0rc2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
